@article{Chin,
 author = {Siew Wen Chin , Li-Minn Ang, Kah Phooi Seng},
 title = {Lips Detection for Audio-Visual Speech Recognition System},
 journal = {Journal of Foo},
 year = 2008
}

@article{Lison:2014:SDS:2677339.2659891,
 author = {Lison, Pierre and Meena, Raveesh},
 title = {Spoken Dialogue Systems: The New Frontier in Human-computer Interaction},
 journal = {XRDS},
 issue_date = {Fall 2014},
 volume = {21},
 number = {1},
 month = oct,
 year = {2014},
 issn = {1528-4972},
 pages = {46--51},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/2659891},
 doi = {10.1145/2659891},
 acmid = {2659891},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{Cooke2001267,
title = "Robust automatic speech recognition with missing and unreliable acoustic data ",
journal = "Speech Communication ",
volume = "34",
number = "3",
pages = "267 - 285",
year = "2001",
note = "",
issn = "0167-6393",
doi = "http://dx.doi.org/10.1016/S0167-6393(00)00034-0",
url = "http://www.sciencedirect.com/science/article/pii/S0167639300000340",
author = "Martin Cooke and Phil Green and Ljubomir Josifovski and Ascension Vizinho",
keywords = "Robust \{ASR\}",
keywords = "Missing data",
keywords = "Data imputation",
keywords = "\{HMM\}",
keywords = "Spectral subtraction "
}

@article{Viseme,
  author    = {Salah Werda and
               Walid Mahdi and
               Abdelmajid Ben Hamadou},
  title     = {Lip Localization and Viseme Classification for Visual Speech Recognition},
  journal   = {CoRR},
  volume    = {abs/1301.4558},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.4558},
  timestamp = {Fri, 01 Feb 2013 13:30:47 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1301-4558},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Dupont,
author={Dupont, S. and Luettin, J.},
journal={Multimedia, IEEE Transactions on},
title={Audio-visual speech modeling for continuous speech recognition},
year={2000},
month={Sep},
volume={2},
number={3},
pages={141-151},
keywords={face recognition;feature extraction;hidden Markov models;learning by example;multimedia systems;sensor fusion;speech recognition;acoustic module;acoustic perceptual linear prediction features;acoustic speech information;appearance-based lip model;audio-visual speech modeling;continuous speech recognition;continuously spoken digits;contour information;example based learning;example images;grey-level information;joint temporal modeling;large multispeaker database;lip movements;lipreading;mouth area;multistream hidden Markov models;noise robust RASTA-PLP;noise-robust features;noisy environments;recognition performance;relative spectra;sensor fusion module;signal-to-noise ratio;speech features;speechreading;temporal dependencies;visual module;visual speech features;visual speech information;Acoustic noise;Data mining;Error analysis;Feature extraction;Hidden Markov models;Noise robustness;Sensor fusion;Signal to noise ratio;Speech recognition;Streaming media},
doi={10.1109/6046.865479},
ISSN={1520-9210},}